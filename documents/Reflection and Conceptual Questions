

We used smaller MLPs because the dataset is tabular and doesnâ€™t need deep networks. Since we used one-hot coded data, Relu performed the best. The goal was to balance learning power and generalization, but obviously, we did not want it to reach over-complexity.




For our model, we used GridSearchCV with 3-fold cross-validation. We also implemented early stopping to track validation accuracy. To help prevent overfitting, we decided to implement L2 regularization, limited epochs, and smaller architectures. The model at the end was evaluated on a 30 percent hold-out test set, just to keep that in mind.



The first thing that comes to mind is how the dataset we worked with has a lot of sensitive attributes, such as race, sex, and marital status. A concern is how models could learn patterns that are unfair to certain people. To make sure to reduce biases, it is important to do fairness checks or maybe even exclude these variables to ensure fairness. 



Since our model has Activation functions, that means it adds non-linearity, this allows the model to learn complex patterns. If we did not have this it would cause multiple layers to collapse and it would become a single linear function. The reason we used Relu is to speed up the training processes all while taking care of the gradients.
